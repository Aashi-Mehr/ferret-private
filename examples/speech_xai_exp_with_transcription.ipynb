{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Set seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "from ferret import SpeechBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'FSC'\n",
    "data_dir = f'{str(Path.home())}/data/speech/fluent_speech_commands_dataset'\n",
    "\n",
    "# We read the test data of FSC dataset\n",
    "df = pd.read_csv(f\"{data_dir}/data/test_data.csv\")\n",
    "df[\"path\"] = df[\"path\"].apply(lambda x: os.path.join(data_dir, x))\n",
    "\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_str = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_str)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"superb/wav2vec2-base-superb-ic\"\n",
    ")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "    \"superb/wav2vec2-base-superb-ic\"\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate benchmark class\n",
    "benchmark = SpeechBenchmark(model, feature_extractor, device=device_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example\n",
    "# 'transcription': 'Turn up the bedroom heat.'\n",
    "# 'action': 'increase'\n",
    "# 'object': 'heat'\n",
    "# 'location': 'bedroom'\n",
    "\n",
    "idx = 136\n",
    "audio_path = dataset[idx]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioSegment.from_wav(audio_path)\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = whisperx.load_model('large-v2', device=device.type, compute_type='float16')\n",
    "asr_alignment_model, metadata = whisperx.load_align_model(language_code='en', device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonaligned_transcriptions = {\n",
    "    idx: asr_model.transcribe(np.array(audio.get_array_of_samples()).astype(np.float32), batch_size=16)\n",
    "}\n",
    "\n",
    "nonaligned_transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions = {\n",
    "    idx: whisperx.align(\n",
    "        nonaligned_transcription[\"segments\"],\n",
    "        asr_alignment_model,\n",
    "        metadata,\n",
    "        np.array(audio.get_array_of_samples()).astype(np.float32),\n",
    "        device.type,\n",
    "        return_char_alignments=False\n",
    "    )\n",
    "    for idx, nonaligned_transcription in nonaligned_transcriptions.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transcriptions is None:\n",
    "    transcriptions_file = f'./transcriptions_{dataset_name}.pickle'\n",
    "    \n",
    "    import pickle\n",
    "    # Load the transcriptions, if available\n",
    "    if os.path.exists(transcriptions_file):\n",
    "        with open(transcriptions_file, \"rb\") as handle:\n",
    "            transcriptions = pickle.load(handle)\n",
    "    else:\n",
    "        transcriptions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_transcript = transcriptions[idx]['word_segments'] if transcriptions else None\n",
    "\n",
    "word_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain word importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = benchmark.explain(\n",
    "    audio_path=audio_path, \n",
    "    methodology='LOO', words_trascript=word_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(benchmark.show_table(explanation, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = benchmark.explain(\n",
    "    audio_path=audio_path, \n",
    "    methodology='LIME', words_trascript=word_transcript)\n",
    "\n",
    "display(benchmark.show_table(explanation, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ferret import AOPC_Comprehensiveness_Evaluation_Speech, AOPC_Sufficiency_Evaluation_Speech\n",
    "\n",
    "aopc_compr = AOPC_Comprehensiveness_Evaluation_Speech(benchmark.model_helper)\n",
    "evaluation_output_c = aopc_compr.compute_evaluation(explanation, words_trascript=word_transcript)\n",
    "\n",
    "aopc_suff = AOPC_Sufficiency_Evaluation_Speech(benchmark.model_helper)\n",
    "evaluation_output_s = aopc_suff.compute_evaluation(explanation, words_trascript=word_transcript)\n",
    "\n",
    "evaluation_output_c, evaluation_output_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain paralinguistic impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_table = benchmark.explain(\n",
    "    audio_path=audio_path,\n",
    "    methodology='perturb_paraling',\n",
    ")\n",
    "display(benchmark.show_table(explain_table, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation_types = ['time stretching', 'pitch shifting', 'reverberation', 'noise']\n",
    "variations_table = benchmark.explain_variations(\n",
    "    audio_path=audio_path,\n",
    "    perturbation_types=perturbation_types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_table_plot = {k:variations_table[k] for k in variations_table if k in ['time stretching', 'pitch shifting', 'noise']}\n",
    "fig = benchmark.plot_variations(variations_table_plot, show_diff = True, figsize=(4.6, 4.2));\n",
    "# fig.savefig(f'example_{dataset_name}_context.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SUPERB - IC Task (FSC).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
